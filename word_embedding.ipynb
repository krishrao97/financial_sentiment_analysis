{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://github.com/buomsoo-kim/Word-embedding-with-Python/blob/master/doc2vec/source%20code/doc2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rose_wrapper.rose import Rose\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import re\n",
    "from nltk import PorterStemmer, WordNetLemmatizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import numpy as np\n",
    "import numpy\n",
    "import sys\n",
    "import getopt\n",
    "import codecs\n",
    "import time\n",
    "import csv\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import gutenberg\n",
    "from multiprocessing import Pool\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/Users/krishnarao/Desktop/Black Snow/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct =  PATH + '/financialdata/'\n",
    "\n",
    "\n",
    "# functions\n",
    "def tokenize(txt):\n",
    "    \"\"\"Simple white space tokenizer.\"\"\"\n",
    "    return txt.split()\n",
    "\n",
    "\n",
    "def scores(row):\n",
    "    \"\"\"Change sentiment groups to numeric values.\"\"\"\n",
    "    if row[\"cla\"] == \"neutral\":\n",
    "        val = 0\n",
    "    elif row[\"cla\"] == \"positive\":\n",
    "        val = 1\n",
    "    else:\n",
    "        val = -1\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "def accuracy(pred, actual):\n",
    "    \"\"\"Compute model accuracy.\"\"\"\n",
    "    return sum(pred == actual) / len(pred)\n",
    "\n",
    "def subsample(df, n, seed):\n",
    "    \"\"\"Create subsample with oversampling.\"\"\"\n",
    "    con = df[\"sentiment\"] == 0\n",
    "    df2 = df.loc[con].sample(n, replace = True, random_state = seed)\n",
    "\n",
    "    con = df[\"sentiment\"] == 1\n",
    "    df2 = df2.append(df.loc[con].sample(n, replace = True, random_state = seed))\n",
    "\n",
    "    con = df[\"sentiment\"] == -1\n",
    "    df2 = df2.append(df.loc[con].sample(n, replace = True, random_state = seed))\n",
    "    return df2\n",
    "\n",
    "\n",
    "# lexical based\n",
    "def wordcount(words, dct):\n",
    "    \"\"\"Count words in dictionary.\"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    counting = Counter(words)\n",
    "    count = []\n",
    "\n",
    "    for key, value in counting.items():\n",
    "        if key in dct:\n",
    "            count.append([key, value])\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def negwordcount(words, dct, negdct, lngram):\n",
    "    \"\"\"Count negated words in dictionary.\"\"\"\n",
    "    from nltk.util import ngrams\n",
    "\n",
    "    mid = int(lngram / 2)\n",
    "    ng = ngrams(words, lngram)\n",
    "    nglist = []\n",
    "\n",
    "    for grams in ng:\n",
    "        nglist.append(grams)\n",
    "\n",
    "    keeper = []\n",
    "    n = len(nglist)\n",
    "    i = 1\n",
    "    for grams in nglist:\n",
    "        if n - i < int(lngram / 2):\n",
    "            mid = mid + 1\n",
    "\n",
    "        if grams[mid] in dct:\n",
    "            for j in grams:\n",
    "                if j in negdct:\n",
    "                    keeper.append(grams[mid])\n",
    "                    break\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    count = wordcount(keeper, dct)\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def findneg(word, wcneg):\n",
    "    \"\"\"Find negation word.\"\"\"\n",
    "    keywordneg = 0\n",
    "\n",
    "    for j in range(0, len(wcneg)):\n",
    "        if word in wcneg[j][0]:\n",
    "            keywordneg = wcneg[j][1]\n",
    "            break\n",
    "\n",
    "    return keywordneg\n",
    "def lexcnt(txt, pos_dct, neg_dct, negat_dct, lngram):\n",
    "    \"\"\"Count words and negated words in dictionary.\"\"\"\n",
    "    from nltk import word_tokenize\n",
    "    txt = word_tokenize(txt)\n",
    "    # Count words in lexicon\n",
    "    pos_wc = wordcount(txt, pos_dct)\n",
    "    pos_wc = [cnt[1] for cnt in pos_wc]\n",
    "    pos_wc = sum(pos_wc)\n",
    "\n",
    "    neg_wc = wordcount(txt, neg_dct)\n",
    "    neg_wc = [cnt[1] for cnt in neg_wc]\n",
    "    neg_wc = sum(neg_wc)\n",
    "\n",
    "    # Count negated words in lexicon\n",
    "    pos_wcneg = negwordcount(txt, pos_dct, negat_dct, lngram)\n",
    "    pos_wcneg = [cnt[1] for cnt in pos_wcneg]\n",
    "    pos_wcneg = sum(pos_wcneg)\n",
    "\n",
    "    neg_wcneg = negwordcount(txt, neg_dct, negat_dct, lngram)\n",
    "    neg_wcneg = [cnt[1] for cnt in neg_wcneg]\n",
    "    neg_wcneg = sum(neg_wcneg)\n",
    "\n",
    "    pos = pos_wc - (pos_wcneg) + neg_wcneg\n",
    "    neg = neg_wc - (neg_wcneg) + pos_wcneg\n",
    "\n",
    "    if pos > neg:\n",
    "        out = 1\n",
    "    elif pos < neg:\n",
    "        out = -1\n",
    "    else:\n",
    "        out = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "# read data\n",
    "\n",
    "# change directory\n",
    "os.chdir(direct + \"financial_phrase_bank\")\n",
    "\n",
    "# load phrasebank\n",
    "phrasebank = pickle.load(open(\"lem_Sentences_66Agree.p\", \"rb\"))\n",
    "\n",
    "sentences = pd.Series(phrasebank[0])\n",
    "classif = pd.Series(phrasebank[1])\n",
    "ident = pd.Series(range(0, len(classif)))\n",
    "\n",
    "df = pd.DataFrame({\"lemma\": sentences,\n",
    "                   \"cla\": classif,\n",
    "                   })\n",
    "\n",
    "df[\"sentiment\"] = df.apply(scores, axis = 1)\n",
    "\n",
    "negat_dct = [\"n't\", \"not\", \"never\", \"no\", \"neither\", \"nor\", \"none\"]\n",
    "lngram = 7\n",
    "\n",
    "\n",
    "\n",
    "# write out test and train data sets\n",
    "# print df\n",
    "train = open((PATH + \"/train.txt\"),\"w+\")\n",
    "test = open((PATH + \"/test.txt\"),\"w+\")\n",
    "for i in range(len(df)):\n",
    "    if not (np.random.rand() >= 0.8):\n",
    "        train.write(\" \"*5 + str(i) + \"\\t\" + df.iloc[i][\"lemma\"] + \"\\t\" + str(df.iloc[i][\"sentiment\"]) + \"\\n\")\n",
    "    else:\n",
    "        test.write(\" \"*5 + str(i) + \"\\t\" + df.iloc[i][\"lemma\"] + \"\\t\" + str(df.iloc[i][\"sentiment\"]) + \"\\n\")\n",
    "train.close()\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /Users/krishnarao/Desktop/Black Snow/data\n",
      "Training data: /Users/krishnarao/Desktop/Black Snow/data/train.txt\n",
      "Test data /Users/krishnarao/Desktop/Black Snow/data/test.txt\n",
      "Vocabulary file: /Users/krishnarao/Desktop/Black Snow/data/v1_vocab_5.txt\n",
      "Finished find_wordcounts for: 3353 docs\n",
      "Finished find_wordcounts for: 864 docs\n",
      "Doc with smallest number of words in vocab has: 0\n",
      "Test Doc with smallest number of words in vocab has: 0\n",
      "Output files: /Users/krishnarao/Desktop/Black Snow/data/v2*\n",
      "Runtime: 5.486937046051025\n"
     ]
    }
   ],
   "source": [
    "WORD_COUNT_THRESHOLD = 5\n",
    "\n",
    "chars = ['{','}','#','%','&','\\(','\\)','\\[','\\]','<','>',',', '!', '.', ';', \n",
    "'?', '*', '\\\\', '\\/', '~', '_','|','=','+','^',':','\\\"','\\'','@','-']\n",
    "\n",
    "def stem(word):\n",
    "   regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "   stem, suffix = re.findall(regexp, word)[0]\n",
    "   return stem\n",
    "\n",
    "def unique(a):\n",
    "   \"\"\" return the list with duplicate elements removed \"\"\"\n",
    "   return list(set(a))\n",
    "\n",
    "def intersect(a, b):\n",
    "   \"\"\" return the intersection of two lists \"\"\"\n",
    "   return list(set(a) & set(b))\n",
    "\n",
    "def union(a, b):\n",
    "   \"\"\" return the union of two lists \"\"\"\n",
    "   return list(set(a) | set(b))\n",
    "\n",
    "def get_files(mypath):\n",
    "   return [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "\n",
    "def get_dirs(mypath):\n",
    "   return [ f for f in listdir(mypath) if isdir(join(mypath,f)) ]\n",
    "\n",
    "def tokenize_corpus(path, train=True):\n",
    "\n",
    "  porter = PorterStemmer() # also lancaster stemmer\n",
    "  wnl = WordNetLemmatizer()\n",
    "  stopWords = stopwords.words(\"english\")\n",
    "  classes = []\n",
    "  samples = []\n",
    "  docs = []\n",
    "  if train == True:\n",
    "    words = {}\n",
    "  f = open(path, 'r')\n",
    "  lines = f.readlines()\n",
    "\n",
    "  for line in lines:\n",
    "    classes.append(line.rsplit()[-1])\n",
    "    samples.append(line.rsplit()[0])\n",
    "    raw = line\n",
    "    raw = ' '.join(raw.rsplit()[1:-1])\n",
    "    # remove noisy characters; tokenize\n",
    "    raw = re.sub('[%s]' % ''.join(chars), ' ', raw)\n",
    "    tokens = word_tokenize(raw)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    tokens = [w for w in tokens if w not in stopWords]\n",
    "    tokens = [wnl.lemmatize(t) for t in tokens]\n",
    "    tokens = [porter.stem(t) for t in tokens]   \n",
    "    if train == True:\n",
    "     for t in tokens: \n",
    "         try:\n",
    "             words[t] = words[t]+1\n",
    "         except:\n",
    "             words[t] = 1\n",
    "    docs.append(tokens)\n",
    "\n",
    "  if train == True:\n",
    "     return(docs, classes, samples, words)\n",
    "  else:\n",
    "     return(docs, classes, samples)\n",
    "\n",
    "def wordcount_filter(words, num=WORD_COUNT_THRESHOLD):\n",
    "   keepset = []\n",
    "   for k in words.keys():\n",
    "       if(words[k] > num):\n",
    "           keepset.append(k)\n",
    "   print (\"Vocab length:\", len(keepset))\n",
    "   return(sorted(set(keepset)))\n",
    "\n",
    "def find_wordcounts(docs, vocab):\n",
    "   bagofwords = numpy.zeros(shape=(len(docs),len(vocab)), dtype=numpy.uint8)\n",
    "   vocabIndex={}\n",
    "   for i in range(len(vocab)):\n",
    "      vocabIndex[vocab[i]]=i\n",
    "\n",
    "   for i in range(len(docs)):\n",
    "       doc = docs[i]\n",
    "\n",
    "       for t in doc:\n",
    "          index_t=vocabIndex.get(t)\n",
    "          if index_t != None:\n",
    "             bagofwords[i,index_t]=bagofwords[i,index_t]+1\n",
    "\n",
    "   print (\"Finished find_wordcounts for:\", len(docs), \"docs\")\n",
    "   return(bagofwords)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "outputf = 'v2'\n",
    "vocabf = 'v1_vocab_' + str(WORD_COUNT_THRESHOLD) + '.txt'\n",
    "\n",
    "path = PATH #'/Users/chris/findata/data'\n",
    "traintxt = path+\"/train.txt\"\n",
    "testtxt  = path+\"/test.txt\"\n",
    "print ('Path:', path)\n",
    "print ('Training data:', traintxt)\n",
    "print ('Test data', testtxt)\n",
    "\n",
    "# Tokenize training data (if training vocab doesn't already exist):\n",
    "if (not vocabf):\n",
    "    word_count_threshold = WORD_COUNT_THRESHOLD\n",
    "    (docs, classes, samples, words) = tokenize_corpus(traintxt, train=True)\n",
    "    (docsTest, classesTest, samplesTest) = tokenize_corpus(testtxt, train=False)\n",
    "    vocab = wordcount_filter(words, num=word_count_threshold)\n",
    "    # Write new vocab file\n",
    "    vocabf = outputf+\"_vocab_\"+str(word_count_threshold)+\".txt\"\n",
    "    outfile = codecs.open(path+\"/\"+vocabf, 'w',\"utf-8-sig\")\n",
    "    outfile.write(\"\\n\".join(vocab))\n",
    "    outfile.close()\n",
    "else:\n",
    "    word_count_threshold = WORD_COUNT_THRESHOLD\n",
    "    (docs, classes, samples) = tokenize_corpus(traintxt, train=False)\n",
    "    (docsTest, classesTest, samplesTest) = tokenize_corpus(testtxt, train=False)\n",
    "    vocabfile = open(path+\"/\"+vocabf, 'r')\n",
    "    vocab = [line.rstrip('\\n') for line in vocabfile]\n",
    "    vocabfile.close()\n",
    "\n",
    "print ('Vocabulary file:', path+\"/\"+vocabf)\n",
    "\n",
    "# Get bag of words:\n",
    "bow = find_wordcounts(docs, vocab)\n",
    "bowTest = find_wordcounts(docsTest, vocab)\n",
    "# Check: sum over docs to check if any zero word counts\n",
    "print (\"Doc with smallest number of words in vocab has:\", min(numpy.sum(bow, axis=1)))\n",
    "print (\"Test Doc with smallest number of words in vocab has:\", min(numpy.sum(bowTest, axis=1)))\n",
    "\n",
    "# Write bow file\n",
    "'''with open(path+\"/\"+outputf+\"_bag_of_words_\"+str(word_count_threshold)+\".csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(bow)'''\n",
    "with open(path+\"/\"+outputf+\"_test_bag_of_words_\"+str(word_count_threshold)+\".csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(bowTest)\n",
    "# Write classes\n",
    "'''outfile = open(path+\"/\"+outputf+\"_classes_\"+str(word_count_threshold)+\".txt\", 'w')\n",
    "outfile.write(\"\\n\".join(classes))\n",
    "outfile.close()'''\n",
    "outfile = open(path+\"/\"+outputf+\"_test_classes_\"+str(word_count_threshold)+\".txt\", 'w')\n",
    "outfile.write(\"\\n\".join(classesTest))\n",
    "outfile.close()\n",
    "\n",
    "# Write samples\n",
    "'''outfile = open(path+\"/\"+outputf+\"_samples_class_\"+str(word_count_threshold)+\".txt\", 'w')\n",
    "outfile.write(\"\\n\".join(samples))\n",
    "outfile.close()'''\n",
    "outfile = open(path+\"/\"+outputf+\"_test_samples_class_\"+str(word_count_threshold)+\".txt\", 'w')\n",
    "outfile.write(\"\\n\".join(samplesTest))\n",
    "outfile.close()\n",
    "\n",
    "print ('Output files:', path+\"/\"+outputf+\"*\")\n",
    "\n",
    "# Runtime\n",
    "print ('Runtime:', str(time.time() - start_time))\n",
    "\n",
    "import csv, numpy as np\n",
    "from numpy import genfromtxt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openTxt(filename):\n",
    "    file = open(filename,'r')\n",
    "    newlist = file.read().split('\\n')\n",
    "    file.close()\n",
    "    return newlist\n",
    "\n",
    "# Open files with Words: \n",
    "vocabList = openTxt(PATH + '/v1_vocab_' + str(WORD_COUNT_THRESHOLD) + '.txt')\n",
    "train = openTxt(PATH + '/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = openTxt(PATH + '/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = [] \n",
    "for elem in train: \n",
    "    tokenized_sent = re.sub(r'\\W+', ' ', elem).split()[1:-1]\n",
    "    train_file.append(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_file.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = TaggedDocument(words = sentences[i], tags = ['sent{}'.format(i)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Doc2Vec(documents = sentences, dm = 1, vector_size = 100, window = 3, min_count = 1, epochs = 10, workers = Pool()._processes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('reuters_sent_1_doc2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('reuters_sent_1_doc2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model.infer_vector(sentences[1][0])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "v2 = model.infer_vector(sentences[300][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8291740417480469"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = []\n",
    "for sentence in sentences:\n",
    "    word_embeddings.append(model.infer_vector(sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_embeddings_1.npy', np.asarray(word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tot_raw = [] \n",
    "for elem in train: \n",
    "    tokenized_sent = elem.split()[-1:]\n",
    "    Y_tot_raw.append(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tot = word_embeddings\n",
    "Y_tot = [int(elem) for elem in sum(Y_tot_raw , [])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(len(Y_tot)):\n",
    "    prop = np.random.rand()\n",
    "    if prop >=0.8:\n",
    "        X_test.append(X_tot[i])\n",
    "        y_test.append(Y_tot[i])\n",
    "    else:\n",
    "        X_train.append(X_tot[i])\n",
    "        y_train.append(Y_tot[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def calcCrossval(model):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    print(\"Cross Val Training Accuracy: \" + str(round(scores.mean(),3)) + \", STD: \" + str(round(scores.std()*2,3)))\n",
    "    return scores\n",
    "\n",
    "# Create a class that contains a model and all its variables\n",
    "# In this case, the 'class' is just a variable of a model\n",
    "# that contains the model's relevant variables such as fpr, tpr, etc. \n",
    "class modDetails:    \n",
    "    def __init__(self, predictionsTrain, predictionsTest, \n",
    "                            classifier, train_cvscores, name):\n",
    "        self.predictionsTrain = predictionsTrain\n",
    "        self.predictionsTest = predictionsTest\n",
    "        self.classifier = classifier\n",
    "        self.train_cvscores = train_cvscores\n",
    "        self.name = name\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def accur(prediction, y):\n",
    "    n1 = len(prediction)\n",
    "    n2 = len(y)\n",
    "    if not (n1 == n2):\n",
    "        return -10000\n",
    "    count = 0\n",
    "    for i in range(n1):\n",
    "        if y[i] == prediction[i]:\n",
    "            count = count + 1\n",
    "    return count/n1\n",
    "\n",
    "def runModel2(model,name):\n",
    "    print(name + '-'*50 + ' \\n')\n",
    "    \n",
    "    # Train the Classifier\n",
    "    # And obtain 10-fold cross validation results of the training\n",
    "    if name.find('GMM') >= 0: # if this is GMM, only fit with X_train\n",
    "        tt = model.fit(X_train)\n",
    "    else:\n",
    "        tt = model.fit(X_train,y_train)\n",
    "    \n",
    "    train_cvscores = calcCrossval(model)\n",
    "    \n",
    "    print(str(type(tt)))\n",
    "    \n",
    "    # Calculate classifier accuracy on trained data and test data\n",
    "    predictionsTrain = tt.predict(X_train)\n",
    "    predictionsTest = tt.predict(X_test)\n",
    "    \n",
    "    print(\"Test accuracy:\", accur(predictionsTest, y_test))\n",
    "    print(\"Train accuracy:\", accur(predictionsTrain, y_train))\n",
    "    \n",
    "    model_info = modDetails(predictionsTrain, predictionsTest, \n",
    "                            tt, train_cvscores, name)\n",
    "    return model_info\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg-L1-------------------------------------------------- \n",
      "\n",
      "Cross Val Training Accuracy: 0.624, STD: 0.049\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "Test accuracy: 0.62339514978602\n",
      "Train accuracy: 0.6300904977375565\n"
     ]
    }
   ],
   "source": [
    "logreg2 = LogisticRegression(C=1, penalty='l1', tol=0.01, solver='saga', multi_class='multinomial')\n",
    "logregL1_info = runModel2(logreg2, 'LogReg-L1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest-------------------------------------------------- \n",
      "\n",
      "Cross Val Training Accuracy: 0.633, STD: 0.064\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "Test accuracy: 0.6148359486447932\n",
      "Train accuracy: 0.6429110105580694\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "rf_info = runModel2(rf, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost-------------------------------------------------- \n",
      "\n",
      "Cross Val Training Accuracy: 0.615, STD: 0.077\n",
      "<class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>\n",
      "Test accuracy: 0.6333808844507846\n",
      "Train accuracy: 0.8009049773755657\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc_info = runModel2(gbc, 'Gradient Boost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
